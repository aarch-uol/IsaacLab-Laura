<<<<<<< HEAD
<<<<<<< HEAD
# Adapted from rsl_rl config
seed: 42
policy: "MlpPolicy"
n_timesteps: !!float 5e7
# For 4 minibatches with 4096 envs
# batch_size = (n_envs * n_steps) / n_minibatches = 32768
n_minibatches: 4
n_steps: 32
gamma: 0.99
learning_rate: !!float 5e-4
ent_coef: 0.0
clip_range: 0.2
n_epochs: 5
gae_lambda: 0.95
max_grad_norm: 1.0
vf_coef: 0.5
policy_kwargs: "dict(
  activation_fn=nn.ELU,
  net_arch=[400, 200, 100],
  optimizer_kwargs=dict(eps=1e-8),
  ortho_init=False,
  )"
=======
=======
>>>>>>> abfba5273e35ca74eb713aa9a0404a6fad7fd5a5
# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml#L245
seed: 42

policy: 'MlpPolicy'
n_timesteps: !!float 5e7
batch_size: 256
n_steps: 512
gamma: 0.99
learning_rate: !!float 2.5e-4
ent_coef: 0.0
clip_range: 0.2
n_epochs: 10
gae_lambda: 0.95
max_grad_norm: 1.0
vf_coef: 0.5
device: "cuda:0"
policy_kwargs: "dict(
                  log_std_init=-1,
                  ortho_init=False,
                  activation_fn=nn.ReLU,
                  net_arch=dict(pi=[256, 256], vf=[256, 256])
                )"
<<<<<<< HEAD
>>>>>>> abfba5273e (Fresh start, no history)
=======
>>>>>>> abfba5273e35ca74eb713aa9a0404a6fad7fd5a5
